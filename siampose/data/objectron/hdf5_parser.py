"""Objectron dataset parser module.

This module contains dataset parsers used to load the HDF5 archive of the Objectron dataset.
See https://github.com/google-research-datasets/Objectron for more info.
"""

import collections
import os
import typing
import time

import cv2 as cv
import h5py
import numpy as np
import pickle
import pytorch_lightning
from torch import tensor
import torch.utils.data
import tqdm
import random

try:
    import turbojpeg

    turbojpeg = turbojpeg.TurboJPEG()
except ImportError:
    turbojpeg = None

import siampose.data.objectron.data_transforms
import siampose.data.objectron.sequence_parser
import siampose.data.objectron.utils
import siampose.data.utils


class ObjectronHDF5SequenceParser(torch.utils.data.Dataset):
    """Objectron HDF5 dataset parser.

    This class can be used to parse the (non-official) Objectron HDF5. More specifically, it allows
    random access over all sequences of the original dataset. See the HDF5 extractor's module for
    information on what the HDF5 files contain.
    """

    def __init__(
        self,
        hdf5_path: typing.AnyStr,
        seq_subset: typing.Optional[
            typing.Sequence[typing.AnyStr]
        ] = None,  # default => use all
    ):
        self.hdf5_path = hdf5_path
        assert os.path.exists(self.hdf5_path), f"invalid dataset path: {self.hdf5_path}"
        all_objects = (
            siampose.data.objectron.sequence_parser.ObjectronSequenceParser.all_objects
        )
        if not seq_subset:
            self.objects = all_objects
        else:
            # assume we provided subset as a list of '<objname>/<seqid>' or '<objname>' strings
            objects = sorted(
                [obj for obj in np.unique([s.split("/")[0] for s in seq_subset])]
            )
            assert all(
                [obj in all_objects for obj in objects]
            ), "invalid object name used in filter"
            self.objects = objects
        self.seq_subset = seq_subset
        self.seq_name_map = {}
        with h5py.File(self.hdf5_path, mode="r") as fd:
            self.target_subsampl_rate = fd.attrs["target_subsampl_rate"]
            self.objectron_subset = fd.attrs["objectron_subset"]
            self.data_fields = fd.attrs["data_fields"]
            self.attr_fields = fd.attrs["attr_fields"]
            for object in self.objects:
                if object not in fd:
                    print(f"missing '{object}' group in dataset, skipping...")
                    continue
                for seq_id, seq_data in fd[object].items():
                    seq_name = object + "/" + seq_id
                    if not self.seq_subset or seq_name in self.seq_subset:
                        self.seq_name_map[len(self.seq_name_map)] = seq_name
        assert (
            len(self.seq_name_map) > 0
        ), "invalid sequence subset specified for given archive"
        if not self.seq_subset:
            self.seq_subset = [s for s in self.seq_name_map.values()]
        self.local_fd = None  # will be opened by worker on first getitem call

    def __len__(self):
        return len(self.seq_name_map)

    def __getitem__(self, idx):
        assert 0 <= idx < len(self.seq_name_map), "sequence index oob"
        if self.local_fd is None:
            self.local_fd = h5py.File(self.hdf5_path, mode="r")
        return self.local_fd[self.seq_name_map[idx]]

    @staticmethod
    def _is_frame_valid(dataset, frame_idx):
        # to keep things light and fast (enough), we'll only look at object centroids
        im_coords = dataset["CENTROID_2D_IM"][frame_idx]
        # in short, if any centroid is outside the image frame by more than its size, it's bad
        # (this will catch crazy-bad object coordinates that would lead to insanely big crops)
        return -640 < im_coords[0] < 1280 and -480 < im_coords[1] < 960

    @staticmethod
    def _decode_jpeg(data):
        if turbojpeg is not None:
            image = turbojpeg.decode(data)
        else:
            image = cv.imdecode(data, cv.IMREAD_COLOR)
        image = cv.cvtColor(image, cv.COLOR_BGR2RGB)
        return image


class ObjectronHDF5FrameTupleParser(ObjectronHDF5SequenceParser):
    """Objectron HDF5 dataset parser.

    This class can be used to parse the (non-official) Objectron HDF5. More specifically, it allows
    random access over frame tuples of the original dataset. See the HDF5 extractor's module for
    information on what the HDF5 files contain.

    What's a tuple in this context, you ask? Well, it's a series of consecutive frames (which
    might already be subsampled in the HDF5 writer) that are also separated by `frame_offset`
    extra frames. Consecutive tuples are then also separated by `tuple_offset` frames.
    """

    # @@@@ TODO in the future: create pairs on smallest pt reproj errors?

    def __init__(
        self,
        hdf5_path: typing.AnyStr,
        seq_subset: typing.Optional[
            typing.Sequence[typing.AnyStr]
        ] = None,  # default => use all
        tuple_length: int = 2,  # by default, we will create pairs of frames
        frame_offset: int = 1,  # by default, we will create tuples from consecutive frames
        tuple_offset: int = 2,  # by default, we will skip a frame between tuples
        keep_only_frames_with_valid_kpts: bool = False,  # set to true when training on kpts
        _target_fields: typing.Optional[typing.List[typing.AnyStr]] = None,
        _transforms: typing.Optional[typing.Any] = None,
        _resort_keypoints: bool = False,
        pairing_strategy="next",
    ):
        # also make a local var for the archive name, as we don't want to overwrite cache for each
        hdf5_name = os.path.basename(
            hdf5_path
        )  # yes, it's normal that this looks "unused" (it's used)
        cache_hash = siampose.data.utils.get_params_hash(
            {k: v for k, v in vars().items() if not k.startswith("_") and k != "self"}
        )
        super().__init__(hdf5_path=hdf5_path, seq_subset=seq_subset)
        assert (
            tuple_length >= 1 and frame_offset >= 1 and tuple_offset >= 1
        ), "invalid tuple params"
        self.tuple_length = tuple_length
        self.frame_offset = frame_offset
        self.tuple_offset = tuple_offset
        self.keep_only_frames_with_valid_kpts = keep_only_frames_with_valid_kpts
        self.target_fields = self.data_fields if not _target_fields else _target_fields
        self.transforms = _transforms
        self.pairing_strategy = pairing_strategy
        if self.pairing_strategy == "next":
            cache_path = os.path.join(os.path.dirname(hdf5_path), cache_hash + ".pkl")
            if os.path.isfile(cache_path):
                with open(cache_path, "rb") as fd:
                    self.frame_pair_map = pickle.load(fd)
            else:
                self.frame_pair_map = self._fetch_tuple_metadata()
                with open(cache_path, "wb") as fd:
                    pickle.dump(self.frame_pair_map, fd)
            self.resort_keypoints = _resort_keypoints
        elif pairing_strategy in ["uniform", "tcn_triplet"]:
            cache_path = os.path.join(
                os.path.dirname(hdf5_path), cache_hash + "_random.pkl"
            )
            if os.path.isfile(cache_path):
                with open(cache_path, "rb") as fd:
                    self.frame_list, self.sequence_map = pickle.load(fd)
            else:
                self.frame_list, self.sequence_map = self._fetch_frame_metadata()
                with open(cache_path, "wb") as fd:
                    pickle.dump((self.frame_list, self.sequence_map), fd)
            self.resort_keypoints = _resort_keypoints
        else:
            raise ValueError(f"Unsuported pairing strategy: {self.pairing_strategy}")

    def _fetch_tuple_metadata(self):
        tuple_map = {}
        assert len(self.seq_subset) > 0
        with h5py.File(self.hdf5_path, mode="r") as fd:
            progress_bar = tqdm.tqdm(
                self.seq_subset,
                total=len(self.seq_subset),
                desc=f"pre-fetching tuple metadata",
            )
            for seq_name in progress_bar:
                seq_data = fd[seq_name]
                seq_len = len(seq_data["IMAGE"])
                if seq_len <= self.frame_offset:  # skip, can't create frame tuples
                    continue
                seq_image_size = (
                    seq_data.attrs["IMAGE_WIDTH"],
                    seq_data.attrs["IMAGE_HEIGHT"],
                )
                # note: the frame idx here is not the real index if the sequence was subsampled
                tuple_start_idx = 0
                while tuple_start_idx < seq_len:
                    candidate_tuple_vals = collections.defaultdict(list)
                    for tuple_idx_offset in range(self.tuple_length):
                        curr_frame_idx = (
                            tuple_start_idx + tuple_idx_offset * self.frame_offset
                        )
                        if curr_frame_idx >= seq_len:
                            break  # if we're oob for any element in the tuple, break it off
                        if not self._is_frame_valid(seq_data, curr_frame_idx):
                            break  # if an element has a bad frame, break it off
                        # we also get the 2d point projections here
                        curr_pts_2d = seq_data["POINT_2D"][curr_frame_idx].reshape(
                            (-1, 3)
                        )[:, :2]
                        assert (
                            len(curr_pts_2d) == 9
                        ), "did we not melt the dataset down to 1 instance?"
                        if self.keep_only_frames_with_valid_kpts:
                            # arbitrary pick: min valid keypoint count = 3
                            good_kpts = np.logical_and(
                                curr_pts_2d <= 1, curr_pts_2d >= 0
                            ).all(axis=1)
                            if sum(good_kpts) < 3:
                                break
                        candidate_tuple_vals["POINT_2D"].append(
                            np.multiply(curr_pts_2d, seq_image_size)
                        )
                        candidate_tuple_vals["frame_idxs"].append(curr_frame_idx)
                    if (
                        len(candidate_tuple_vals["frame_idxs"]) == self.tuple_length
                    ):  # if it's all good, keep it
                        candidate_tuple_vals["seq_name"] = seq_name
                        tuple_map[len(tuple_map)] = dict(candidate_tuple_vals)
                    tuple_start_idx += self.tuple_offset
        return tuple_map

    def _fetch_frame_metadata(self):
        frame_list = []
        sequence_map = {}
        assert len(self.seq_subset) > 0
        with h5py.File(self.hdf5_path, mode="r") as fd:
            progress_bar = tqdm.tqdm(
                self.seq_subset,
                total=len(self.seq_subset),
                desc=f"pre-fetching tuple metadata",
            )
            for seq_name in progress_bar:
                if seq_name not in sequence_map:
                    sequence_map[seq_name] = []  # Create empty list
                seq_data = fd[seq_name]
                seq_len = len(seq_data["IMAGE"])
                if seq_len <= self.frame_offset:  # skip, can't create frame tuples
                    continue
                seq_image_size = (
                    seq_data.attrs["IMAGE_WIDTH"],
                    seq_data.attrs["IMAGE_HEIGHT"],
                )
                # note: the frame idx here is not the real index if the sequence was subsampled
                tuple_start_idx = 0
                while tuple_start_idx < seq_len:  # TODO: Replace by for loop.
                    candidate_frame = collections.defaultdict(list)
                    # for tuple_idx_offset in range(self.tuple_length):
                    curr_frame_idx = tuple_start_idx
                    if curr_frame_idx >= seq_len:
                        break  # if we're oob for any element in the tuple, break it off
                    if not self._is_frame_valid(seq_data, curr_frame_idx):
                        break  # if an element has a bad frame, break it off
                    # we also get the 2d point projections here
                    curr_pts_2d = seq_data["POINT_2D"][curr_frame_idx].reshape((-1, 3))[
                        :, :2
                    ]
                    assert (
                        len(curr_pts_2d) == 9
                    ), "did we not melt the dataset down to 1 instance?"
                    if self.keep_only_frames_with_valid_kpts:
                        # arbitrary pick: min valid keypoint count = 3
                        good_kpts = np.logical_and(
                            curr_pts_2d <= 1, curr_pts_2d >= 0
                        ).all(axis=1)
                        if sum(good_kpts) < 3:
                            break
                    candidate_frame["POINT_2D"].append(
                        np.multiply(curr_pts_2d, seq_image_size)
                    )
                    candidate_frame["frame_idxs"].append(curr_frame_idx)
                    # if len(candidate_tuple_vals["frame_idxs"]) == self.tuple_length:  # if it's all good, keep it
                    candidate_frame["seq_name"] = seq_name
                    frame_list.append(dict(candidate_frame))
                    sequence_map[seq_name].append(dict(candidate_frame))
                    tuple_start_idx += 1
        return frame_list, sequence_map

    def __len__(self):
        if self.pairing_strategy == "next":
            return len(self.frame_pair_map)
        elif self.pairing_strategy in ["uniform", "tcn_triplet"]:
            return len(self.frame_list)
        else:
            raise NotImplementedError

    def __getitem__(self, idx):
        if self.local_fd is None:
            self.local_fd = h5py.File(self.hdf5_path, mode="r")
        if (
            self.pairing_strategy == "next"
        ):  # TODO: Intelligent refactor. So much code duplication!
            assert 0 <= idx < len(self.frame_pair_map), "frame pair index oob"
            meta = self.frame_pair_map[idx]
            seq_name = meta["seq_name"]
            seq_data = self.local_fd[seq_name]
            sample = {
                field: np.stack(
                    [
                        self._decode_jpeg(seq_data[field][frame_idx])
                        if field == "IMAGE"
                        else seq_data[field][frame_idx]
                        for frame_idx in meta["frame_idxs"]
                    ]
                )
                for field in self.target_fields
            }
            uid = f"hdf5_{seq_name}_" + str(seq_data["IMAGE_ID"][meta["frame_idxs"][0]])
            sample = {
                "SEQ_IDX": seq_name,
                "UID": uid,  # TODO: Harmonize Unique Identifiers for evaluation.
                "FRAME_REAL_IDXS": np.asarray(
                    [seq_data["IMAGE_ID"][idx] for idx in meta["frame_idxs"]]
                ),
                "FRAME_IDXS": np.asarray(meta["frame_idxs"]),
                "POINTS": np.pad(
                    np.asarray(meta["POINT_2D"]), ((0, 0), (0, 0), (0, 2))
                ),  # pad 2d kpts for augments
                "CAT_ID": self.objects.index(seq_name.split("/")[0]),
                **sample,
            }
        elif (
            self.pairing_strategy == "uniform"
        ):  # TODO: Intelligent refactor. So much code duplication!
            assert 0 <= idx < len(self.frame_list), "frame index oob"
            meta = self.frame_list[idx]
            seq_name = meta["seq_name"]
            seq_data = self.local_fd[seq_name]
            # import random
            other_frame = random.sample(self.sequence_map[seq_name], 1)[
                0
            ]  # Uniform random sample.
            if len(meta["frame_idxs"]) > 1:
                print(f"To many fram idxs for sample {seq_name}/{meta['frame_idxs']}")
            if len(other_frame["frame_idxs"]) > 1:
                print(
                    f"To many fram idxs for sample {seq_name}/{other_frame['frame_idxs']}"
                )
            meta["frame_idxs"] = [meta["frame_idxs"][0]]
            meta["POINT_2D"] = [meta["POINT_2D"][0]]  # Handle rarely occuring strays
            other_frame["frame_idxs"] = [other_frame["frame_idxs"][0]]
            other_frame["POINT_2D"] = [
                other_frame["POINT_2D"][0]
            ]  # Handle rarely occuring strays
            assert len(meta["frame_idxs"]) == 1
            assert len(other_frame["frame_idxs"]) == 1

            meta["frame_idxs"].append(other_frame["frame_idxs"][0])
            meta["POINT_2D"].append(other_frame["POINT_2D"][0])
            sample = {
                field: np.stack(
                    [
                        self._decode_jpeg(seq_data[field][frame_idx])
                        if field == "IMAGE"
                        else seq_data[field][frame_idx]
                        for frame_idx in meta["frame_idxs"]
                    ]
                )
                for field in self.target_fields
            }

            uid = f"hdf5_{seq_name}_" + str(seq_data["IMAGE_ID"][meta["frame_idxs"][0]])
            sample = {
                "SEQ_IDX": seq_name,
                "UID": uid,  # TODO: Harmonize Unique Identifiers for evaluation.
                "FRAME_REAL_IDXS": np.asarray(
                    [seq_data["IMAGE_ID"][idx] for idx in meta["frame_idxs"]]
                ),
                "FRAME_IDXS": np.asarray(meta["frame_idxs"]),
                "POINTS": np.pad(
                    np.asarray(meta["POINT_2D"]), ((0, 0), (0, 0), (0, 2))
                ),  # pad 2d kpts for augments
                "CAT_ID": self.objects.index(seq_name.split("/")[0]),
                **sample,
            }
            assert len(sample["FRAME_IDXS"]) == 2
            assert len(sample["FRAME_REAL_IDXS"]) == 2
            assert len(sample["POINTS"]) == 2
            assert len(sample["IMAGE"]) == 2
        elif (
            self.pairing_strategy == "tcn_triplet"
        ):  # TODO: Intelligent refactor. So much code duplication!
            assert 0 <= idx < len(self.frame_list), "frame index oob"
            meta = self.frame_list[idx]
            frame_idx = meta["frame_idxs"][0]
            seq_name = meta["seq_name"]
            seq_data = self.local_fd[seq_name]
            # From TCN paper:
            # Positive range: 0.2s. At 5 fps, this is the previous or next frame.
            # Margin = 2xpositive range = +-0.4s. That is any other frame.
            pos_range = 1  # 0.2s
            margin = 2  # 0.4s
            direction = random.choice([True, False])
            if direction:  # Forward in time.
                if (frame_idx + pos_range) < len(self.sequence_map[seq_name]):  #
                    positive_frame = self.sequence_map[seq_name][frame_idx + pos_range]
                else:
                    positive_frame = self.sequence_map[seq_name][
                        frame_idx - pos_range
                    ]  # This will rarely occur. We will use previous instead of next.
            else:
                if frame_idx != 0:
                    positive_frame = self.sequence_map[seq_name][frame_idx - pos_range]
                else:
                    positive_frame = self.sequence_map[seq_name][
                        0 + pos_range
                    ]  # This will rarely occur. We will use the next instead of the previous
            # import random
            for i in range(0, 4):  # The negative can be any other frame.
                negative_frame = random.sample(self.sequence_map[seq_name], 1)[
                    0
                ]  # Uniform random sample for the negative.
                negative_frame_idx = negative_frame["frame_idxs"][0]
                if abs(frame_idx - negative_frame_idx) > margin:
                    break  # Our search is complete!
            # It could happen that we dont find a good negative within 3 attempt, but this is highly
            # unlikely and should be considered "noise" that won't affect the model.

            if len(meta["frame_idxs"]) > 1:
                print(f"To many fram idxs for sample {seq_name}/{meta['frame_idxs']}")
            if len(positive_frame["frame_idxs"]) > 1:
                print(
                    f"To many fram idxs for sample {seq_name}/{positive_frame['frame_idxs']}"
                )
            if len(negative_frame["frame_idxs"]) > 1:
                print(
                    f"To many fram idxs for sample {seq_name}/{negative_frame['frame_idxs']}"
                )
            meta["frame_idxs"] = [meta["frame_idxs"][0]]
            meta["POINT_2D"] = [meta["POINT_2D"][0]]  # Handle rarely occuring strays
            positive_frame["frame_idxs"] = [positive_frame["frame_idxs"][0]]
            positive_frame["POINT_2D"] = [
                positive_frame["POINT_2D"][0]
            ]  # Handle rarely occuring strays
            negative_frame["frame_idxs"] = [negative_frame["frame_idxs"][0]]
            negative_frame["POINT_2D"] = [
                negative_frame["POINT_2D"][0]
            ]  # Handle rarely occuring strays
            assert len(meta["frame_idxs"]) == 1
            assert len(positive_frame["frame_idxs"]) == 1

            meta["frame_idxs"].append(positive_frame["frame_idxs"][0])
            meta["POINT_2D"].append(positive_frame["POINT_2D"][0])

            meta["frame_idxs"].append(negative_frame["frame_idxs"][0])
            meta["POINT_2D"].append(negative_frame["POINT_2D"][0])

            sample = {
                field: np.stack(
                    [
                        self._decode_jpeg(seq_data[field][frame_idx])
                        if field == "IMAGE"
                        else seq_data[field][frame_idx]
                        for frame_idx in meta["frame_idxs"]
                    ]
                )
                for field in self.target_fields
            }

            uid = f"hdf5_{seq_name}_" + str(seq_data["IMAGE_ID"][meta["frame_idxs"][0]])
            sample = {
                "SEQ_IDX": seq_name,
                "UID": uid,  # TODO: Harmonize Unique Identifiers for evaluation.
                "FRAME_REAL_IDXS": np.asarray(
                    [seq_data["IMAGE_ID"][idx] for idx in meta["frame_idxs"]]
                ),
                "FRAME_IDXS": np.asarray(meta["frame_idxs"]),
                "POINTS": np.pad(
                    np.asarray(meta["POINT_2D"]), ((0, 0), (0, 0), (0, 2))
                ),  # pad 2d kpts for augments
                "CAT_ID": self.objects.index(seq_name.split("/")[0]),
                **sample,
            }
            assert len(sample["FRAME_IDXS"]) == 3
            assert len(sample["FRAME_REAL_IDXS"]) == 3
            assert len(sample["POINTS"]) == 3
            assert len(sample["IMAGE"]) == 3
        else:
            raise NotImplementedError
        if self.transforms is not None:
            sample = self.transforms(sample)
        # resort after transforms so flips can be used
        if self.resort_keypoints:
            sample[
                "sorting_good"
            ] = siampose.data.objectron.utils.sort_sample_keypoints(sample)
        return sample


class ObjectronFramePairDataModule(pytorch_lightning.LightningDataModule):

    name = "objectron_frame_pair"

    def __init__(
        self,
        hdf5_path: typing.AnyStr,
        tuple_length: int = 2,
        frame_offset: int = 1,
        tuple_offset: int = 2,
        keep_only_frames_with_valid_kpts: bool = False,
        input_height: int = 224,
        gaussian_blur: bool = True,
        jitter_strength: float = 1.0,
        valid_split_ratio: float = 0.1,
        num_workers: int = 8,
        batch_size: int = 256,
        split_seed: int = 1337,
        pin_memory: bool = True,
        drop_last: bool = False,
        use_hflip_augment: bool = True,
        shared_transform: bool = True,
        crop_height: typing.Union[int, typing.AnyStr] = "auto",
        crop_scale: tuple = (0.2, 1),
        crop_ratio: tuple = (0.75, 1.33),
        crop_strategy: str = "centroid",
        sync_hflip=False,
        resort_keypoints: bool = False,
        pairing_strategy: str = "next",
        *args: typing.Any,
        **kwargs: typing.Any,
    ):
        super().__init__(*args, **kwargs)
        self.image_size = input_height
        self.dims = (3, input_height, input_height)
        self.gaussian_blur = gaussian_blur
        self.jitter_strength = jitter_strength
        self.valid_split_ratio = valid_split_ratio
        self.hdf5_path = hdf5_path
        self.tuple_length = tuple_length
        self.frame_offset = frame_offset
        self.tuple_offset = tuple_offset
        self.keep_only_frames_with_valid_kpts = keep_only_frames_with_valid_kpts
        self.num_workers = num_workers
        self.batch_size = batch_size
        self.split_seed = (
            split_seed if split_seed is not None else 1337
        )  # keep this static between runs
        self.pin_memory = pin_memory
        self.drop_last = drop_last
        self.use_hflip_augment = use_hflip_augment
        self.shared_transform = shared_transform
        self.crop_height = crop_height
        self.crop_scale = crop_scale
        self.crop_ratio = crop_ratio
        self.crop_strategy = crop_strategy
        self.sync_hflip = sync_hflip
        self.resort_keypoints = resort_keypoints
        self.pairing_strategy = pairing_strategy
        # create temp dataset to get total sequence/sample count
        dataset = ObjectronHDF5FrameTupleParser(
            hdf5_path=self.hdf5_path,
            seq_subset=None,  # grab all sequences in first pass, then use em for split
            tuple_length=self.tuple_length,
            frame_offset=self.frame_offset,
            tuple_offset=self.tuple_offset,
            keep_only_frames_with_valid_kpts=self.keep_only_frames_with_valid_kpts,
            pairing_strategy=self.pairing_strategy,
        )
        # we do the split in a 2nd pass and re-parse the subsets
        # ...caching will be 2x longer, but it's the easiest solution, and it costs ~5 minutes once
        pre_seed_state = np.random.get_state()
        np.random.seed(self.split_seed)
        seq_subset_idxs = np.random.permutation(len(dataset.seq_subset))
        np.random.set_state(pre_seed_state)
        self.valid_seq_count = int(len(seq_subset_idxs) * self.valid_split_ratio)
        self.train_seq_count = len(seq_subset_idxs) - self.valid_seq_count
        self.valid_seq_subset = sorted(
            [dataset.seq_subset[idx] for idx in seq_subset_idxs[: self.valid_seq_count]]
        )
        self.train_seq_subset = sorted(
            [dataset.seq_subset[idx] for idx in seq_subset_idxs[self.valid_seq_count :]]
        )
        if self.pairing_strategy == "next":
            self.valid_sample_subset = [
                idx
                for idx, m in dataset.frame_pair_map.items()
                if m["seq_name"] in self.valid_seq_subset
            ]
            self.train_sample_subset = [
                idx
                for idx, m in dataset.frame_pair_map.items()
                if m["seq_name"] in self.train_seq_subset
            ]
        elif self.pairing_strategy in ["uniform", "tcn_triplet"]:
            self.valid_sample_subset = [
                idx
                for idx, m in enumerate(dataset.frame_list)
                if m["seq_name"] in self.valid_seq_subset
            ]
            self.train_sample_subset = [
                idx
                for idx, m in enumerate(dataset.frame_list)
                if m["seq_name"] in self.train_seq_subset
            ]
        else:
            raise ValueError(f"Unsupported pairing stragegy: {self.pairing_strategy}")
        self.valid_sample_count = len(self.valid_sample_subset)
        self.train_sample_count = len(self.train_sample_subset)
        assert len(np.intersect1d(self.valid_seq_subset, self.train_seq_subset)) == 0
        assert (
            len(np.intersect1d(self.valid_sample_count, self.train_sample_count)) == 0
        )
        if "_test" not in self.hdf5_path:
            assert self.train_sample_count > 0 and self.valid_sample_count > 0
        else:
            assert self.valid_sample_count > 0
        self.train_dataset = None  # will be initialized once setup is called
        self.val_dataset = None

    def setup(self, stage=None):
        if "val_transforms" not in self.__dict__:  # only setup once
            self.val_transforms = self.val_transform()
        if "train_transforms" not in self.__dict__:  # only setup once
            self.train_transforms = self.train_transform()
        target_fields = [
            "IMAGE",
            "CENTROID_2D_IM",
            "POINT_3D",
            "PROJECTION_MATRIX",
            "VIEW_MATRIX",
            "PLANE_CENTER",
            "PLANE_NORMAL",
        ]
        self.val_dataset = ObjectronHDF5FrameTupleParser(
            hdf5_path=self.hdf5_path,
            seq_subset=self.valid_seq_subset,
            tuple_length=self.tuple_length,
            frame_offset=self.frame_offset,
            tuple_offset=self.tuple_offset,
            keep_only_frames_with_valid_kpts=self.keep_only_frames_with_valid_kpts,
            _target_fields=target_fields,
            _transforms=self.val_transforms,
            _resort_keypoints=self.resort_keypoints,
            pairing_strategy=self.pairing_strategy,
        )
        if "_test" in self.hdf5_path:
            return  # No train set to setup if were are using the test set!
        self.train_dataset = ObjectronHDF5FrameTupleParser(
            hdf5_path=self.hdf5_path,
            seq_subset=self.train_seq_subset,
            tuple_length=self.tuple_length,
            frame_offset=self.frame_offset,
            tuple_offset=self.tuple_offset,
            keep_only_frames_with_valid_kpts=self.keep_only_frames_with_valid_kpts,
            _target_fields=target_fields,
            _transforms=self.train_transforms,
            _resort_keypoints=self.resort_keypoints,
            pairing_strategy=self.pairing_strategy,
        )

    def train_dataloader(self, evaluation=False) -> torch.utils.data.DataLoader:
        if evaluation:  # We don't want transformations for finding nearest neighbors.
            self.train_dataset.transforms = self.val_transforms
        return torch.utils.data.DataLoader(
            self.train_dataset,
            batch_size=self.batch_size,
            shuffle=True,
            num_workers=self.num_workers,
            drop_last=self.drop_last,
            pin_memory=self.pin_memory,
        )

    def val_dataloader(self, evaluation=False) -> torch.utils.data.DataLoader:
        return torch.utils.data.DataLoader(
            self.val_dataset,
            batch_size=self.batch_size,
            sampler=siampose.data.utils.ConstantRandomOrderSampler(self.val_dataset),
            shuffle=False,
            num_workers=self.num_workers,
            drop_last=self.drop_last,
            pin_memory=self.pin_memory,
        )

    def train_transform(self):
        return (
            siampose.data.objectron.data_transforms.SimSiamFramePairTrainDataTransform(
                crop_height=self.crop_height,
                input_height=self.image_size,
                gaussian_blur=self.gaussian_blur,
                jitter_strength=self.jitter_strength,
                crop_scale=self.crop_scale,
                crop_ratio=self.crop_ratio,
                use_hflip_augment=self.use_hflip_augment,
                shared_transform=self.shared_transform,
                crop_strategy=self.crop_strategy,
                sync_hflip=self.sync_hflip,
            )
        )

    def val_transform(self):
        return (
            siampose.data.objectron.data_transforms.SimSiamFramePairEvalDataTransform(
                crop_height=self.crop_height,
                input_height=self.image_size,
                gaussian_blur=self.gaussian_blur,
                jitter_strength=self.jitter_strength,
                crop_scale=self.crop_scale,
                crop_ratio=self.crop_ratio,
                use_hflip_augment=self.use_hflip_augment,
                shared_transform=self.shared_transform,
                crop_strategy=self.crop_strategy,
                sync_hflip=False,  # No flipping during validation!!!!
                augmentation=False,
            )
        )


if __name__ == "__main__":
    data_path = "/home/raphael/datasets/objectron/"
    hdf5_path = data_path + "extract_s5_raw.hdf5"
    config_path = "examples/local/config-pretrain-8gb-cyclic-dryrun.yaml"
    display_keypoints = True
    batch_size = 1
    max_iters = 100000

    import yaml

    with open(config_path, "r") as stream:
        hyper_params = yaml.load(stream, Loader=yaml.FullLoader)

    import siampose.utils.reproducibility_utils

    hyper_params["seed"] = 0
    siampose.utils.reproducibility_utils.set_seed(hyper_params["seed"])

    dm = ObjectronFramePairDataModule(
        hdf5_path=hdf5_path,
        tuple_length=hyper_params.get("tuple_length"),
        frame_offset=hyper_params.get("frame_offset"),
        tuple_offset=hyper_params.get("tuple_offset"),
        keep_only_frames_with_valid_kpts=hyper_params.get(
            "keep_only_frames_with_valid_kpts"
        ),
        input_height=hyper_params.get("input_height"),
        gaussian_blur=hyper_params.get("gaussian_blur"),
        jitter_strength=hyper_params.get("jitter_strength"),
        batch_size=batch_size,
        num_workers=0,
        use_hflip_augment=hyper_params.get("use_hflip_augment"),
        shared_transform=hyper_params.get("shared_transform"),
        crop_height=hyper_params.get("crop_height"),
        crop_scale=(
            hyper_params.get("crop_scale_min"),
            hyper_params.get("crop_scale_max"),
        ),
        crop_ratio=(
            hyper_params.get("crop_ratio_min"),
            hyper_params.get("crop_ratio_max"),
        ),
        crop_strategy=hyper_params.get("crop_strategy"),
        sync_hflip=hyper_params.get("sync_hflip"),
        resort_keypoints=hyper_params.get("resort_keypoints"),
    )
    assert dm.train_sample_count > 0
    dm.setup()
    loader = dm.val_dataloader()
    norm_std = np.asarray([0.229, 0.224, 0.225]).reshape((1, 1, 3))
    norm_mean = np.asarray([0.485, 0.456, 0.406]).reshape((1, 1, 3))
    # display = batch_size == 1
    display = False
    iter = 0
    init_time = time.time()
    for batch in tqdm.tqdm(loader, total=len(loader)):
        crops = batch["OBJ_CROPS"]
        assert len(crops) == 3
        for crop in crops:
            assert crop.shape == torch.Size([1, 3, 224, 224])
        if display:
            display_frames = []
            for frame_idx, (frame, pts, pts_3d) in enumerate(
                zip(batch["OBJ_CROPS"], batch["POINTS"], batch["POINT_3D"])
            ):
                # de-normalize and ready image for opencv display to show the result of transforms
                frame = (
                    (
                        (frame.squeeze(0).numpy().transpose((1, 2, 0)) * norm_std)
                        + norm_mean
                    )
                    * 255
                ).astype(np.uint8)
                for pt_idx, pt in enumerate(pts.view(-1, 2)):
                    pt = (int(round(pt[0].item())), int(round(pt[1].item())))
                    color = (32, 224, 32) if pt_idx == 0 else (32, 32, 224)
                    frame = cv.circle(
                        frame.copy(), pt, radius=3, color=color, thickness=-1
                    )
                    cv.putText(
                        frame, f"{pt_idx}", pt, cv.FONT_HERSHEY_SIMPLEX, 0.5, color
                    )
                display_frames.append(frame)
            # print(f"sorting_good = {batch['sorting_good']}")
            cv.imshow(
                f"frames",
                cv.resize(
                    cv.hconcat(display_frames),
                    dsize=(-1, -1),
                    fx=4,
                    fy=4,
                    interpolation=cv.INTER_NEAREST,
                ),
            )
            cv.waitKey(0)
        iter += 1
        if max_iters is not None and iter > max_iters:
            break
    print(f"all done in {time.time() - init_time} seconds")
